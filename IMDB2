# -*- coding:utf-8 -*-
import pandas as pd
import nltk.data
from bs4 import BeautifulSoup
import re
from nltk.corpus import stopwords
from gensim.models import word2vec
from gensim.models import Word2Vec
import numpy as np
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.grid_search import GridSearchCV
import csv

def review_to_text(review,remove_stopwords):
    raw_text = BeautifulSoup(review, "html.parser").get_text()
    letters = re.sub('[^a-zA-Z]',' ',raw_text)
    words = letters.lower().split()
    if remove_stopwords:
        stop_words = set(stopwords.words('english'))
        words = [w for w in words if w not in stop_words]
    return words

def review_to_sentences(review, tokenizer):
    raw_sentences = tokenizer.tokenize(review.strip( ))
    sentences = []
    for raw_sentence in raw_sentences:
        if len(raw_sentence)>0:
            sentences.append(review_to_text(raw_sentence, False)) # 返回的数据以句子为单位

    return sentences

def makeFeatureVec(words, model, num_features):
    featureVec = np.zeros((num_features,),dtype="float32")#创建一个300行1列的零矩阵，数据类型为float32
    nwords = 0.
    index2word_set = set(model.index2word)
    for word in words:
        if word in index2word_set:
            nwords = nwords+1.
            featureVec = np.add(featureVec,model[word])# 将出现在文本中的词向量相加
    featureVec = np.divide(featureVec,nwords) #计算出向量的平均值代表文本向量
    return featureVec

def getAvgFeatureVecs(reviews, model, num_features):
    counter = 0
    reviewFeatureVecs = np.zeros((len(reviews), num_features),dtype="float32")#创建一个len(reviews)行300列的零矩阵
    for review in reviews:
        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)
        counter +=1
    return reviewFeatureVecs #得到所有文本的矩阵向量并返回

def cos(vector1,vector2):
    dot_product = 0.0;
    normA = 0.0;
    normB = 0.0;
    for a,b in zip(vector1,vector2):
        dot_product += a*b
        normA += a**2
        normB += b**2
    if normA == 0.0 or normB==0.0:
        return None
    else:
        return dot_product / ((normA*normB)**0.5)

if __name__ == '__main__':
    # unlabeled_train = pd.read_csv('C:\Users\Administrator\Desktop\IMDB\unlabeledTrainData.tsv', delimiter='\t', quoting=3)
    # print unlabeled_train.head( )
    train = pd.read_csv('C:\Users\Administrator\Desktop\IMDB\labeledTrainData.tsv', delimiter='\t')
    test = pd.read_csv('C:\Users\Administrator\Desktop\IMDB\TestData.tsv', delimiter='\t')
    y_train = train['sentiment']
    # tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
    # corpora = []
    # for review in unlabeled_train['review']:
    #     corpora += review_to_sentences(review.decode('utf8'), tokenizer)

    # print len(corpora[0])
    # f=open('C:\Users\Administrator\Desktop\IMDB\demo3.txt','w')
    # for i in range(0,len(corpora[0])):
    #     f.write(corpora[0][i])
    # f.write('\n')
    # for i in range(0, len(corpora[1])):
    #     f.write(corpora[1][i])
    # f.write('\n')
    # for i in range(0, len(corpora[2])):
    #     f.write(corpora[2][i])
    # f.close()
    # 构建词向量模型时，输入为每一个句子
    num_features = 300
    # min_word_count = 20
    # num_workers = 4
    # context = 10
    # downsampling = 1e-3
    # model = word2vec.Word2Vec(corpora, workers=num_workers, \
    #                           size=num_features, min_count=min_word_count, \
    #                           window=context, sample=downsampling)
    #
    # model.init_sims(replace=True)
    # model_name = "C:\Users\Administrator\Desktop\IMDB\wordvec"
    # model.save(model_name)

    model = Word2Vec.load("C:\Users\Administrator\Desktop\IMDB\wordvec")
    # print model.most_similar("man")

    clean_train_reviews =[]
    for review in train["review"]:
        clean_train_reviews.append(review_to_text(review,remove_stopwords=True))

    trainDataVecs = getAvgFeatureVecs(clean_train_reviews, model, num_features)
    # 保存训练文本的文章向量到本地
    # Train_vec = tuple(map(tuple, trainDataVecs))
    # output = pd.DataFrame(data={"ID":train["id"], "VECTOR":Train_vec})
    # output.to_csv("C:\Users\Administrator\Desktop\IMDB\Train_vec.csv", index=False, quoting=csv.QUOTE_ALL)

    clean_test_reviews = []
    for review in test["review"]:
        clean_test_reviews.append(review_to_text(review, remove_stopwords=True))

    testDataVecs = getAvgFeatureVecs(clean_test_reviews, model, num_features)
    # 保存测试文本的文章向量到本地
    # Test_vec = tuple(map(tuple, testDataVecs))
    # output = pd.DataFrame(data={"ID": test["id"], "VECTOR": Test_vec})
    # output.to_csv("C:\Users\Administrator\Desktop\IMDB\Test_vec.csv", index=False, quoting=csv.QUOTE_ALL)

    # 测试（test）
    # min_distance=0
    # for i in trainDataVecs:
    #     distance = cos(trainDataVecs[0],trainDataVecs[i])

    # 使用梯度提升树作为分类器建模，并对测试文本进行分类
    # gbc = GradientBoostingClassifier( )
    # params_gbc = {'n_estimators':[10, 100, 500], 'learning_rate':[0.01, 0.1, 1.0], 'max_depth':[2, 3, 4]}
    # gs = GridSearchCV(gbc, params_gbc, cv=4, n_jobs=-1, verbose=1)
    # gs.fit(trainDataVecs,y_train)
    #
    # print gs.best_score_
    # print gs.best_params_
    #
    # result = gs.predict(testDataVecs)
    # output = pd.DataFrame( data={"id":test["id"], "sentiment":result} )
    # output.to_csv("C:\Users\Administrator\Desktop\IMDB\w2v_result.csv", index=False, quoting=3)


